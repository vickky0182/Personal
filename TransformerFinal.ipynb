{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vickky0182/Personal/blob/main/TransformerFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eipc2nzNp3ja",
        "outputId": "9b21e31a-44db-4c2b-fded-c2b4182ff495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory.used [MiB]\n",
            "0 MiB\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi --query-gpu=memory.used --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uotsEr0Id6x",
        "outputId": "5271215d-a948-4f17-c881-e0b768d82059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCpuw2x6M9ex"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/My Drive/Colab Notebooks/new_dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTCp4NyILxVL"
      },
      "outputs": [],
      "source": [
        "fault_paths = [file_path + f\"fault{i}/\" for i in range(12)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWjkK4hTrm0F"
      },
      "outputs": [],
      "source": [
        "def remove_zero(i,a):\n",
        "  target_column = a\n",
        "  min_value = i[target_column].min()\n",
        "  max_value = i[target_column].max()\n",
        "  random_values = np.random.uniform(min_value, max_value, len(i))\n",
        "  i.loc[i[target_column] == 0, target_column] = random_values[i[target_column] == 0]\n",
        "  return i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3k4Zn28srsp"
      },
      "outputs": [],
      "source": [
        "def add_rows(a,c,n):\n",
        "  lis = a.values.tolist()\n",
        "  lower_limit = a[c].min()  # Lower limit of the range\n",
        "  upper_limit = a[c].max()  # Upper limit of the range\n",
        "  for i in range(n):\n",
        "    random_int = random.randint(int(lower_limit), int(upper_limit))\n",
        "    lis.append([random_int])\n",
        "  new=pd.DataFrame(lis, columns=[c])\n",
        "  return new\n",
        "  # target_column = c\n",
        "  # median_value = a[target_column].median()\n",
        "  # a.loc[a[target_column] == 0, target_column] = median_value\n",
        "  # return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sqZ40FDkYaf"
      },
      "outputs": [],
      "source": [
        "def read_file(a,b,f):\n",
        "  c=a+b+\"/\"\n",
        "\n",
        "  column_names = ['wastage', 'Throttle_Pos_Area(%)', 'engine_speed', 'Pressure', 'Temp']\n",
        "  i=c+b+\"_inputdata.txt\"\n",
        "  input = pd.read_csv(i, delimiter='\\s+', header=None, names=column_names)\n",
        "\n",
        "\n",
        "  column_names = ['Engine_speed']\n",
        "  o=c+b+\"_omega_eREFdata.txt\"\n",
        "  omega = pd.read_csv(o, delimiter='\\s+', header=None, names=column_names)\n",
        "\n",
        "  column_names = ['Engine_Torque']\n",
        "  t=c+b+\"_Tq_eREFdata.txt\"\n",
        "  torque = pd.read_csv(t, delimiter='\\s+', header=None, names=column_names)\n",
        "\n",
        "  column_names = [\"Tmp_AF\",\"P_AF\",\"Tmp_CMP\",\"P_CMP\",\"Tmp_IC\",\"P_IC\",\"Tmp_IN_Man\",\"P_IN_Man\",\"Tmp_EX_Man\",\"P_Ex_Man\",\"Tmp_Turb\",\"P_Turb\",\"Turb_Speed\"]\n",
        "  s=c+b+\"_statedata.txt\"\n",
        "  state = pd.read_csv(s, delimiter='\\s+', header=None, names=column_names)\n",
        "\n",
        "  column_names = ['TMP_Cmp','P_Cp','TMP_IC','P_IC','TMP_IN_Man','P_IN_Man','AF_MassFlow','E_Torque','P_EX_Man']\n",
        "  ou=c+b+\"_outputdata.txt\"\n",
        "  output =pd.read_csv(ou, delimiter='\\s+', header=None, names=column_names)\n",
        "\n",
        "  input=remove_zero(input,'engine_speed')\n",
        "\n",
        "  torque=add_rows(torque,'Engine_Torque',len(input)-len(torque))\n",
        "  omega=add_rows(omega,'Engine_speed',len(input)-len(omega))\n",
        "\n",
        "  df_input = pd.DataFrame(input.values, columns=['wastage', 'Throttle_Pos_Area(%)', 'engine_speed', 'Pressure', 'Temp'])\n",
        "  df_state = pd.DataFrame(state.values, columns=[\"Tmp_AF\",\"P_AF\",\"Tmp_CMP\",\"P_CMP\",\"Tmp_IC\",\"P_IC\",\"Tmp_IN_Man\",\"P_IN_Man\",\"Tmp_EX_Man\",\"P_Ex_Man\",\"Tmp_Turb\",\"P_Turb\",\"Turb_Speed\"])\n",
        "  df_output = pd.DataFrame(output.values, columns=['TMP_Cmp','P_Cp','TMP_IC','P_IC','TMP_IN_Man','P_IN_Man','AF_MassFlow','E_Torque','P_EX_Man'])\n",
        "  df_new_omega = pd.DataFrame(omega.values, columns=['Engine_speed'])\n",
        "  df_new_torque = pd.DataFrame(torque.values, columns=['Engine_Torque'])\n",
        "  df_fault = pd.DataFrame([f]*len(input), columns=['Fault'])\n",
        "\n",
        "  df_input=df_input.drop('engine_speed', axis=1)\n",
        "  df_output=df_output.drop('E_Torque', axis=1)\n",
        "\n",
        "  merged_df = pd.concat([df_input, df_state, df_output, df_new_omega, df_new_torque,df_fault], axis=1)\n",
        "\n",
        "  return merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaokdaKpL9Wx"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47CumoIXj8S4",
        "outputId": "a8620f8f-5fbb-418e-be06-21404ddf3c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fault0 read Successfully\n",
            "Fault1 read Successfully\n",
            "Fault2 read Successfully\n",
            "Fault3 read Successfully\n",
            "Fault4 read Successfully\n",
            "Fault5 read Successfully\n",
            "Fault6 read Successfully\n",
            "Fault7 read Successfully\n",
            "Fault8 read Successfully\n",
            "Fault9 read Successfully\n",
            "Fault10 read Successfully\n",
            "Fault11 read Successfully\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for fault_path in fault_paths:\n",
        "\n",
        "    fault = int(fault_path.split(\"fault\")[-1].replace(\"/\", \"\"))  # Extract the fault number from the path\n",
        "\n",
        "    wltp_folders = [folder for folder in os.listdir(fault_path) if not folder.startswith('.')]  # Exclude files or folders starting with a dot\n",
        "\n",
        "    for wltp_folder in wltp_folders:\n",
        "        wltp_folder_path = fault_path + wltp_folder  # Path to the WLTP folder\n",
        "        df = read_file(fault_path, wltp_folder, fault)  # Read the data from the WLTP folder\n",
        "        combined_df = pd.concat([combined_df, df], axis=0)  # Append the data to the combined DataFrame\n",
        "    print(\"Fault\"+ str(fault) +\" read Successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu5jhMg16uMD"
      },
      "outputs": [],
      "source": [
        "fault = combined_df['Fault']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc0669GX69tl"
      },
      "outputs": [],
      "source": [
        "combined_df = combined_df.drop('Fault', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcx14ymP3C2o",
        "outputId": "5d51ed21-0178-425a-c569-84b042c4afc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         0\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         0\n",
              "         ..\n",
              "47316    11\n",
              "47317    11\n",
              "47318    11\n",
              "47319    11\n",
              "47320    11\n",
              "Name: Fault, Length: 6446294, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "fault"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqUo5HbS6iya"
      },
      "outputs": [],
      "source": [
        "merged_tensor = torch.tensor(combined_df.values)\n",
        "fault_tensor = torch.tensor(fault.values,dtype=merged_tensor.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA362ynvlw5J",
        "outputId": "071bbcfd-b926-46ea-cc05-39b10fa27a93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6446294, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "merged_tensor.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oul9Y_OzI0ai"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnawH6f37UOn"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjx89F5uEu2Q"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, num_heads, dropout):\n",
        "        super(EncoderDecoderTransformer, self).__init__()\n",
        "\n",
        "        self.transformer = nn.Transformer(d_model=input_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, merged_tensor):\n",
        "        transformed = self.transformer(merged_tensor, merged_tensor)\n",
        "        output = self.fc(transformed)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDtan6Pq3v_h"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, merged_tensor, fault_tensor):\n",
        "        self.merged_tensor = merged_tensor\n",
        "        self.fault_tensor = fault_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.merged_tensor.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        merged_row = self.merged_tensor[idx]\n",
        "        fault_label = self.fault_tensor[idx]\n",
        "        return merged_row, fault_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb7CfILx3y39"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, train_loader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for merged_row, fault_label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        merged_row = merged_row.to(device).float()  # Convert to float\n",
        "        fault_label = fault_label.to(device).long()  # Convert to long\n",
        "\n",
        "        output = model(merged_row)\n",
        "        loss = criterion(output, fault_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        predicted = torch.argmax(output, dim=1)\n",
        "        correct += (predicted == fault_label).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    accuracy = correct / len(train_loader.dataset)\n",
        "    return train_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcipLKSq33vq"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, criterion, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for merged_row, fault_label in val_loader:\n",
        "            merged_row = merged_row.to(device).float()  # Convert to float\n",
        "            fault_label = fault_label.to(device).long()  # Convert to long\n",
        "\n",
        "            output = model(merged_row)\n",
        "            loss = criterion(output, fault_label)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            predicted = torch.argmax(output, dim=1)\n",
        "            correct += (predicted == fault_label).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "    return val_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJO0SZ1QBULA"
      },
      "outputs": [],
      "source": [
        "input_dim = 27\n",
        "output_dim = 12 # Number of fault classes\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "num_heads = 9\n",
        "dropout = 0.01\n",
        "learning_rate = 0.0001\n",
        "batch_size = 512\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOAMzIFQBUll"
      },
      "outputs": [],
      "source": [
        "model = EncoderDecoderTransformer(input_dim, output_dim, hidden_dim, num_layers, num_heads, dropout).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNADmmayBdH6"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create train and test datasets\n",
        "merged_train, merged_test, fault_train, fault_test = train_test_split(\n",
        "    merged_tensor, fault_tensor, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create train and validation datasets\n",
        "train_dataset = CustomDataset(merged_train, fault_train)\n",
        "test_dataset = CustomDataset(merged_test, fault_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJO_ZZc0oem5"
      },
      "outputs": [],
      "source": [
        "# Create train and validation data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78xgOA7DtwMV"
      },
      "outputs": [],
      "source": [
        "loss = []\n",
        "va_loss =[]\n",
        "acc = []\n",
        "v_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIbK6E2cBf_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caecbd81-67a9-409c-ab55-13a613749fa5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 2.3033 - Train Accuracy: 0.1774\n",
            "Val Loss: 2.1219 - Val Accuracy: 0.2431\n",
            "---------------------------------------------\n",
            "Epoch 2/10\n",
            "Train Loss: 2.0474 - Train Accuracy: 0.2652\n",
            "Val Loss: 1.9327 - Val Accuracy: 0.3004\n",
            "---------------------------------------------\n",
            "Epoch 3/10\n",
            "Train Loss: 1.9427 - Train Accuracy: 0.2945\n",
            "Val Loss: 1.8819 - Val Accuracy: 0.3112\n",
            "---------------------------------------------\n",
            "Epoch 4/10\n",
            "Train Loss: 1.8949 - Train Accuracy: 0.3084\n",
            "Val Loss: 1.8563 - Val Accuracy: 0.3213\n",
            "---------------------------------------------\n",
            "Epoch 5/10\n",
            "Train Loss: 1.8675 - Train Accuracy: 0.3174\n",
            "Val Loss: 1.8417 - Val Accuracy: 0.3264\n",
            "---------------------------------------------\n",
            "Epoch 6/10\n",
            "Train Loss: 1.8473 - Train Accuracy: 0.3251\n",
            "Val Loss: 1.8145 - Val Accuracy: 0.3373\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_accuracy = train(model, criterion, optimizer, train_loader)\n",
        "    val_loss, val_accuracy = evaluate(model, criterion, test_loader)\n",
        "    loss.append(train_loss)\n",
        "    va_loss.append(val_loss)\n",
        "    acc.append(train_accuracy)\n",
        "    v_acc.append(val_accuracy)\n",
        "    print(f'Epoch {epoch}/{num_epochs}')\n",
        "    print(f'Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4PN5-vmt9VO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy0KqFKoFjIa"
      },
      "outputs": [],
      "source": [
        "# Plotting the Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), loss, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs + 1), va_loss, label='Val Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print()\n",
        "print()\n",
        "# Plotting the Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), acc, label='Train Accuracy')\n",
        "plt.plot(range(1, num_epochs + 1), v_acc, label='Val Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Jlx2AzIRrE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzYHMW_3IS58"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}